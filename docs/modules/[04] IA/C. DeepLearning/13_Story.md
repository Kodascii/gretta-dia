<link rel="stylesheet" href="../../stylesheet.css">

# Story_13 — NER (Named Entity Recognition)

## Links
- [spaCy](https://course.spacy.io/fr/chapter4)
- [Word embedding techniques](https://dataaspirant.com/word-embedding-techniques-nlp/#t-1597685144197)

## Contexte
> Apprendre les techniques de Deep Learning pour reconnaître des entités nommées.

## Mots clefs
- <def-of>Morphosyntaxique</def-of> : *L'analyse morphosyntaxique consiste à étudier la structure et la formation des mots dans une phrase, y compris leur morphologie (formes) et leur syntaxe (ordre et relations grammaticales).*
- <def-of>NER (Named Entity Recognition)</def-of> : *Technique utilisée pour identifier et classer des types spécifiques de mots dans un texte, tels que les noms de personnes, d'organisations, de lieux, de dates, de valeurs monétaires, etc.*
- <def-of>Word Embedding</def-of> : *Représentation vectorielle des mots qui capture leurs relations sémantiques et syntaxiques.*
- <def-of>Word2Vec</def-of> : *Modèle d'apprentissage automatique qui génère des représentations vectorielles de mots en fonction de leur contexte dans un texte.*
- <def-of>Skip-gram</def-of> : *Prédit le mot courant en fonction des mots qui l'entourent dans une fenêtre de contexte donnée.*
- <def-of>CBOW (Continuous Bag-of-Words)</def-of> : *Technique d'apprentissage automatique utilisée pour générer des représentations de mots.*
- <def-of>GloVe</def-of> : *Méthode d'intégration de mots basée sur un réseau neuronal qui combine les avantages de la factorisation matricielle et des méthodes de fenêtre de contexte local. GloVe construit une matrice de cooccurrence mot-mot et apprend les incorporations en minimisant la différence entre le produit scalaire des vecteurs de mots et le logarithme de leur nombre de cooccurrences.*
- <def-of>BERT (Bidirectional Encoder Representations from Transformers)</def-of> : *Modèle de langage pré-entraîné qui génère des intégrations de mots contextuelles. Il exploite l'architecture Transformer et un objectif de modélisation de langage masqué pour apprendre des représentations bidirectionnelles approfondies, ce qui le rend très efficace pour diverses tâches NLP.*
- <def-of>Ontologie</def-of> : *Représentation formelle d'un domaine de connaissance, comprenant des concepts, leurs relations et leurs propriétés.*
- <def-of>Web Sémentique</def-of> : *Vise à rendre les données du Web plus compréhensibles par les machines, en ajoutant des métadonnées sémantiques aux pages Web.*
- <def-of>LDA (Latent Dirichlet Allocation)</def-of> : *Permet découvrir des thèmes cachés (topics en anglais) dans des collections de documents textuels.*
- <def-of>LSA</def-of> : **
- <def-of>FastText</def-of> : *Extension du modèle Word2Vec, qui représente les mots comme la somme de leurs vecteurs de sous-mots (n-grammes). Cette technique permet à FastText de capturer des informations morphologiques et de générer des intégrations pour les mots hors vocabulaire.*
- <def-of>Gensim</def-of> : **
- <def-of>POS TAG</def-of> : **
- <def-of>Text Mining</def-of> : **
- <def-of>LSTM (Long Short Term Memory)</def-of> : *Type de réseau récurrent (RNN) capable de capturer des dépendances longue distance.*
- <def-of>Transformer</def-of> : **
- <def-of>NNMF (None-Negatif Matrix Factorisation)</def-of> : *Consiste à factoriser une matrice V en deux matrices W et H, de sorte que toutes les matrices aient des éléments non négatifs.*

## Problématiques
1. ?

## Hypothèses
- <u>Le deep learning est obligatoire pour le NLP</u> <h-t/> : *!;*

## Plan d'action
1. Investigation des ressources 
1. Définitions des mots clefs 
1. Vérifier les hypothèses 
1. Répondre aux questions 
1. Tableau de comparaison entre NLTK et SpaCy 
1. Faire Workshop NLKT puis SpaCy 

# RER